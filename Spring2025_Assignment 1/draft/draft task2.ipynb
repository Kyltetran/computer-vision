{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import cv2\n",
    "# import numpy as np\n",
    "# import matplotlib.pyplot as plt\n",
    "\n",
    "# # Load ArUco dictionary\n",
    "# dictionary = cv2.aruco.getPredefinedDictionary(cv2.aruco.DICT_6X6_250)\n",
    "# parameters = cv2.aruco.DetectorParameters()\n",
    "\n",
    "# # Open the main video file\n",
    "# cap_main = cv2.VideoCapture(\"video.mp4\")\n",
    "\n",
    "# while cap_main.isOpened():\n",
    "#     ret, frame = cap_main.read()\n",
    "#     if not ret:\n",
    "#         break  # Stop when video ends\n",
    "\n",
    "#     # Convert to grayscale for marker detection\n",
    "#     gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "#     corners, ids, _ = cv2.aruco.detectMarkers(gray, dictionary, parameters=parameters)\n",
    "\n",
    "#     if ids is not None and len(ids) >= 4:\n",
    "#         print(\"Detected Marker IDs:\", ids.flatten())\n",
    "\n",
    "#         # Extract ROI corner points\n",
    "#         roi_corners = {}\n",
    "#         for marker_id, corner_set in zip(ids.flatten(), corners):\n",
    "#             roi_corners[marker_id] = corner_set[0]\n",
    "\n",
    "#         # Ensure all 4 required markers exist\n",
    "#         required_ids = [23, 25, 30, 33]\n",
    "#         if all(marker_id in roi_corners for marker_id in required_ids):\n",
    "#             ref_pt1 = roi_corners[23][0]  # Top-left\n",
    "#             ref_pt2 = roi_corners[25][1]  # Top-right\n",
    "#             ref_pt3 = roi_corners[30][2]  # Bottom-right\n",
    "#             ref_pt4 = roi_corners[33][3]  # Bottom-left\n",
    "\n",
    "#             # Define ROI points\n",
    "#             pts_dst = np.array([\n",
    "#                 [ref_pt1[0], ref_pt1[1]],\n",
    "#                 [ref_pt2[0], ref_pt2[1]],\n",
    "#                 [ref_pt3[0], ref_pt3[1]],\n",
    "#                 [ref_pt4[0], ref_pt4[1]]\n",
    "#             ], dtype=np.float32)\n",
    "\n",
    "#             print(\"Destination ROI Points:\", pts_dst)\n",
    "\n",
    "#     # Display detected markers\n",
    "#     frame_detected = frame.copy()\n",
    "#     cv2.aruco.drawDetectedMarkers(frame_detected, corners, ids)\n",
    "    \n",
    "#     plt.figure(figsize=[10, 10])\n",
    "#     plt.axis('off')\n",
    "#     plt.imshow(cv2.cvtColor(frame_detected, cv2.COLOR_BGR2RGB))\n",
    "#     plt.show()\n",
    "\n",
    "# cap_main.release()\n",
    "# cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Source Video Points: [[  0.   0.]\n",
      " [637.   0.]\n",
      " [637. 844.]\n",
      " [  0. 844.]]\n"
     ]
    }
   ],
   "source": [
    "# Open the source video (piano.mp4)\n",
    "cap_piano = cv2.VideoCapture(\"piano.mp4\")\n",
    "\n",
    "if not cap_piano.isOpened():\n",
    "    print(\"Error: Could not open piano.mp4.\")\n",
    "    exit()\n",
    "\n",
    "# Read the first frame\n",
    "ret_piano, frame_piano = cap_piano.read()\n",
    "\n",
    "if not ret_piano or frame_piano is None:\n",
    "    print(\"Error: Could not read a valid frame from piano.mp4.\")\n",
    "    cap_piano.release()\n",
    "    exit()\n",
    "\n",
    "# Compute detected ROI dimensions\n",
    "roi_width = int(np.linalg.norm(ref_pt1 - ref_pt2))\n",
    "roi_height = int(np.linalg.norm(ref_pt1 - ref_pt3))\n",
    "\n",
    "# Compute aspect ratios\n",
    "roi_aspect_ratio = roi_width / roi_height\n",
    "src_height, src_width, _ = frame_piano.shape\n",
    "src_aspect_ratio = src_width / src_height\n",
    "\n",
    "# Crop source video to match aspect ratio\n",
    "if src_aspect_ratio > roi_aspect_ratio:\n",
    "    new_width = int(src_height * roi_aspect_ratio)\n",
    "    crop_x = (src_width - new_width) // 2\n",
    "    frame_piano = frame_piano[:, crop_x:crop_x + new_width]\n",
    "elif src_aspect_ratio < roi_aspect_ratio:\n",
    "    new_height = int(src_width / roi_aspect_ratio)\n",
    "    crop_y = (src_height - new_height) // 2\n",
    "    frame_piano = frame_piano[crop_y:crop_y + new_height, :]\n",
    "\n",
    "# Resize after cropping\n",
    "frame_piano = cv2.resize(frame_piano, (roi_width, roi_height))\n",
    "\n",
    "# Define Source Points\n",
    "pts_src = np.array([\n",
    "    [0, 0], \n",
    "    [roi_width, 0], \n",
    "    [roi_width, roi_height], \n",
    "    [0, roi_height]\n",
    "], dtype=np.float32)\n",
    "\n",
    "print(\"Source Video Points:\", pts_src)\n",
    "\n",
    "cap_piano.release()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Open the main video file\n",
    "cap_main = cv2.VideoCapture(\"video.mp4\")\n",
    "\n",
    "# Check if video is opened successfully\n",
    "if not cap_main.isOpened():\n",
    "    print(\"Error: Could not open video.mp4. Check if the file exists and is playable.\")\n",
    "    exit()\n",
    "\n",
    "# Read the first frame\n",
    "ret_main, frame = cap_main.read()\n",
    "\n",
    "if not ret_main or frame is None:\n",
    "    print(\"Error: Could not read a valid frame from video.mp4.\")\n",
    "    cap_main.release()\n",
    "    exit()\n",
    "\n",
    "cap_main.release()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "if frame_piano is None:\n",
    "    print(\"Error: Source video frame is not loaded correctly.\")\n",
    "    exit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'M' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[6]\u001b[39m\u001b[32m, line 3\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# Ensure both frames exist\u001b[39;00m\n\u001b[32m      2\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m frame \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m frame_piano \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m----> \u001b[39m\u001b[32m3\u001b[39m     warped_piano = cv2.warpPerspective(frame_piano, \u001b[43mM\u001b[49m, (frame.shape[\u001b[32m1\u001b[39m], frame.shape[\u001b[32m0\u001b[39m]))\n\u001b[32m      5\u001b[39m     \u001b[38;5;66;03m# Display warped video\u001b[39;00m\n\u001b[32m      6\u001b[39m     plt.figure(figsize=(\u001b[32m10\u001b[39m, \u001b[32m10\u001b[39m))\n",
      "\u001b[31mNameError\u001b[39m: name 'M' is not defined"
     ]
    }
   ],
   "source": [
    "# Ensure both frames exist\n",
    "if frame is not None and frame_piano is not None:\n",
    "    warped_piano = cv2.warpPerspective(frame_piano, M, (frame.shape[1], frame.shape[0]))\n",
    "\n",
    "    # Display warped video\n",
    "    plt.figure(figsize=(10, 10))\n",
    "    plt.imshow(cv2.cvtColor(warped_piano, cv2.COLOR_BGR2RGB))\n",
    "    plt.title(\"Warped Source Video\")\n",
    "    plt.axis(\"off\")\n",
    "    plt.show()\n",
    "else:\n",
    "    print(\"Error: Either frame or frame_piano is None. Cannot proceed with warping.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "frame: [[[198 198 198]\n",
      "  [198 198 198]\n",
      "  [198 198 198]\n",
      "  ...\n",
      "  [122 141 140]\n",
      "  [122 141 140]\n",
      "  [123 143 141]]\n",
      "\n",
      " [[198 198 198]\n",
      "  [198 198 198]\n",
      "  [198 198 198]\n",
      "  ...\n",
      "  [122 141 140]\n",
      "  [122 141 140]\n",
      "  [122 141 140]]\n",
      "\n",
      " [[198 198 198]\n",
      "  [198 198 198]\n",
      "  [198 198 198]\n",
      "  ...\n",
      "  [120 140 139]\n",
      "  [122 141 140]\n",
      "  [122 141 140]]\n",
      "\n",
      " ...\n",
      "\n",
      " [[192 188 186]\n",
      "  [192 188 186]\n",
      "  [190 187 185]\n",
      "  ...\n",
      "  [  5  23  39]\n",
      "  [  5  23  39]\n",
      "  [  5  23  39]]\n",
      "\n",
      " [[193 189 187]\n",
      "  [192 188 186]\n",
      "  [190 187 185]\n",
      "  ...\n",
      "  [  2  19  35]\n",
      "  [  1  18  34]\n",
      "  [  1  18  34]]\n",
      "\n",
      " [[193 189 187]\n",
      "  [192 188 186]\n",
      "  [190 187 185]\n",
      "  ...\n",
      "  [  0  16  32]\n",
      "  [  0  15  31]\n",
      "  [  0  15  31]]]\n",
      "frame_piano: [[[ 79  82  79]\n",
      "  [ 79  82  79]\n",
      "  [ 79  82  79]\n",
      "  ...\n",
      "  [103 108 100]\n",
      "  [103 108 100]\n",
      "  [103 108 100]]\n",
      "\n",
      " [[ 79  82  79]\n",
      "  [ 79  82  79]\n",
      "  [ 79  82  79]\n",
      "  ...\n",
      "  [103 108 100]\n",
      "  [103 108 100]\n",
      "  [103 108 100]]\n",
      "\n",
      " [[ 79  82  79]\n",
      "  [ 79  82  79]\n",
      "  [ 79  82  79]\n",
      "  ...\n",
      "  [103 108 100]\n",
      "  [103 108 100]\n",
      "  [103 108 100]]\n",
      "\n",
      " ...\n",
      "\n",
      " [[  4   1   5]\n",
      "  [  4   1   5]\n",
      "  [  4   1   5]\n",
      "  ...\n",
      "  [ 33  37  33]\n",
      "  [ 33  37  33]\n",
      "  [ 33  37  33]]\n",
      "\n",
      " [[  4   1   5]\n",
      "  [  4   1   5]\n",
      "  [  4   1   5]\n",
      "  ...\n",
      "  [ 33  37  33]\n",
      "  [ 33  37  33]\n",
      "  [ 33  37  33]]\n",
      "\n",
      " [[  4   1   5]\n",
      "  [  4   1   5]\n",
      "  [  4   1   5]\n",
      "  ...\n",
      "  [ 33  37  33]\n",
      "  [ 33  37  33]\n",
      "  [ 33  37  33]]]\n"
     ]
    }
   ],
   "source": [
    "print(\"frame:\", frame)\n",
    "print(\"frame_piano:\", frame_piano)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'NoneType' object has no attribute 'shape'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mAttributeError\u001b[39m                            Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[3]\u001b[39m\u001b[32m, line 5\u001b[39m\n\u001b[32m      2\u001b[39m M = cv2.getPerspectiveTransform(pts_src, pts_dst)\n\u001b[32m      4\u001b[39m \u001b[38;5;66;03m# Warp the source video frame\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m5\u001b[39m warped_piano = cv2.warpPerspective(frame_piano, M, (\u001b[43mframe\u001b[49m\u001b[43m.\u001b[49m\u001b[43mshape\u001b[49m[\u001b[32m1\u001b[39m], frame.shape[\u001b[32m0\u001b[39m]))\n\u001b[32m      7\u001b[39m \u001b[38;5;66;03m# Display the warped video frame\u001b[39;00m\n\u001b[32m      8\u001b[39m plt.figure(figsize=(\u001b[32m10\u001b[39m, \u001b[32m10\u001b[39m))\n",
      "\u001b[31mAttributeError\u001b[39m: 'NoneType' object has no attribute 'shape'"
     ]
    }
   ],
   "source": [
    "# Compute the transformation matrix\n",
    "M = cv2.getPerspectiveTransform(pts_src, pts_dst)\n",
    "\n",
    "# Warp the source video frame\n",
    "warped_piano = cv2.warpPerspective(frame_piano, M, (frame.shape[1], frame.shape[0]))\n",
    "\n",
    "# Display the warped video frame\n",
    "plt.figure(figsize=(10, 10))\n",
    "plt.imshow(cv2.cvtColor(warped_piano, cv2.COLOR_BGR2RGB))\n",
    "plt.title(\"Warped Source Video\")\n",
    "plt.axis(\"off\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a mask for blending\n",
    "mask = np.zeros((frame.shape[0], frame.shape[1]), dtype=np.uint8)\n",
    "cv2.fillConvexPoly(mask, np.int32([pts_dst]), (255, 255, 255), cv2.LINE_AA)\n",
    "\n",
    "# Convert mask to 3 channels\n",
    "mask3 = np.zeros_like(frame, dtype=np.uint8)\n",
    "for i in range(3):\n",
    "    mask3[:, :, i] = mask\n",
    "\n",
    "# Blend images\n",
    "frame_out = cv2.bitwise_and(frame, cv2.bitwise_not(mask3))  # Remove original region\n",
    "frame_out += cv2.bitwise_and(warped_piano, mask3)  # Overlay warped video\n",
    "\n",
    "# Display the final result\n",
    "cv2.imshow(\"Warped Video Overlay\", frame_out)\n",
    "cv2.waitKey(0)\n",
    "cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'cv2' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# Open the main video and source video\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m cap_main = \u001b[43mcv2\u001b[49m.VideoCapture(\u001b[33m\"\u001b[39m\u001b[33mvideo.mp4\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m      3\u001b[39m cap_piano = cv2.VideoCapture(\u001b[33m\"\u001b[39m\u001b[33mpiano.mp4\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m      5\u001b[39m \u001b[38;5;66;03m# Check if both videos opened successfully\u001b[39;00m\n",
      "\u001b[31mNameError\u001b[39m: name 'cv2' is not defined"
     ]
    }
   ],
   "source": [
    "# Open the main video and source video\n",
    "cap_main = cv2.VideoCapture(\"video.mp4\")\n",
    "cap_piano = cv2.VideoCapture(\"piano.mp4\")\n",
    "\n",
    "# Check if both videos opened successfully\n",
    "if not cap_main.isOpened() or not cap_piano.isOpened():\n",
    "    print(\"Error: Could not open one or both video files.\")\n",
    "    exit()\n",
    "\n",
    "while cap_main.isOpened() and cap_piano.isOpened():\n",
    "    ret_main, frame_main = cap_main.read()\n",
    "    ret_piano, frame_piano = cap_piano.read()\n",
    "\n",
    "    # Stop if either video ends\n",
    "    if not ret_main or not ret_piano:\n",
    "        break  \n",
    "\n",
    "    # Check if frame_piano is valid\n",
    "    if frame_piano is None:\n",
    "        print(\"Error: Source video frame is empty.\")\n",
    "        continue\n",
    "\n",
    "    # Resize the source video frame\n",
    "    roi_width = int(np.linalg.norm(ref_pt1 - ref_pt2))\n",
    "    roi_height = int(np.linalg.norm(ref_pt1 - ref_pt3))\n",
    "    frame_piano = cv2.resize(frame_piano, (roi_width, roi_height))\n",
    "\n",
    "    # Define source and destination points\n",
    "    pts_src = np.array([\n",
    "        [0, 0], \n",
    "        [roi_width, 0], \n",
    "        [roi_width, roi_height], \n",
    "        [0, roi_height]\n",
    "    ], dtype=np.float32)\n",
    "\n",
    "    pts_dst = np.array([\n",
    "        [ref_pt1[0], ref_pt1[1]],  \n",
    "        [ref_pt2[0], ref_pt2[1]],  \n",
    "        [ref_pt3[0], ref_pt3[1]],  \n",
    "        [ref_pt4[0], ref_pt4[1]]   \n",
    "    ], dtype=np.float32)\n",
    "\n",
    "    # Compute homography and warp the frame\n",
    "    M = cv2.getPerspectiveTransform(pts_src, pts_dst)\n",
    "    warped_piano = cv2.warpPerspective(frame_piano, M, (frame_main.shape[1], frame_main.shape[0]))\n",
    "\n",
    "    # Display debug info\n",
    "    print(\"Warping frame from piano.mp4\")\n",
    "    \n",
    "    # Show the result\n",
    "    cv2.imshow(\"Warped Video Overlay\", warped_piano)\n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        break  \n",
    "\n",
    "cap_main.release()\n",
    "cap_piano.release()\n",
    "cv2.destroyAllWindows()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cv-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
